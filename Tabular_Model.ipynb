{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "fix_random(42)\n",
    "\n",
    "if not os.path.exists('tabularML'):\n",
    "    os.makedirs('tabularML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# PyTorch Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print only the year column\n",
    "df.drop(['year','rating_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9934\n",
      "Number of validation samples: 1104\n",
      "Number of testing samples: 2760\n",
      "\n",
      "Number of features: 1148\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "Y = df['rating']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_val = Y_val.to_numpy()\n",
    "Y_test = Y_test.to_numpy()\n",
    "\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_val = Y_val.reshape(-1, 1)\n",
    "print(f'Number of training samples: {X_train.shape[0]}')\n",
    "print(f'Number of validation samples: {X_val.shape[0]}')\n",
    "print(f'Number of testing samples: {X_test.shape[0]}')\n",
    "print(f'\\nNumber of features: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperparameter combinations: 1\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "nums_epochs = [100]\n",
    "batch_sizes = [64]\n",
    "patience = [10]\n",
    "n_d_a = [16]\n",
    "n_shared = [3]\n",
    "n_indipendent = [3]\n",
    "n_step = [5]\n",
    "gamma = [1.3]\n",
    "epsilon = [1e-8]\n",
    "\n",
    "hyperparameters = itertools.product(n_d_a, n_step,n_indipendent,n_shared, gamma, epsilon,nums_epochs, batch_sizes)\n",
    "n_comb = len(n_d_a)*len(n_step)*len(n_indipendent)*len(n_shared)*len(gamma)*len(epsilon)*len(nums_epochs)*len(batch_sizes)\n",
    "print (f'Number of hyperparameter combinations: {n_comb}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_d_a, n_step,n_indipendent,n_shared, gamma, epsilon):\n",
    "    model = TabNetRegressor(\n",
    "        # n_d: the dimensionality of the output space of the feature transformer network (default 64)\n",
    "        n_d=n_d_a,\n",
    "        # n_a: the dimensionality of the output space of the attention network (default 64)\n",
    "        n_a=n_d_a,\n",
    "        # n_steps: the number of sequential steps in the attention mechanism (default 3)\n",
    "        n_steps=n_step,\n",
    "        # gamma: the scaling factor for the feature transformer network (default 1.3)\n",
    "        gamma=gamma,\n",
    "        # optimizerm name of optimizer to use (default Adam)\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        # n_independent: the number of independent feature transformer networks to use (default 2)\n",
    "        n_independent=n_indipendent,\n",
    "        # n_shared: the number of shared feature transformer networks to use (default 2)\n",
    "        n_shared=n_shared,\n",
    "        # epsilon: a small value to add to the denominator of the feature importance calculation to avoid division by zero (default 1e-15)\n",
    "        epsilon=epsilon,\n",
    "        # seed: the random seed to use for reproducibility (default None)\n",
    "        seed=42    \n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterations 1/1 - Hyperparameters:  batch_sizes=64, nums_epochs=100, n_d=16, n_a=16, n_step=5, n_indipendent=3, n_shared=3, gamma=1.3, epsilon=1e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.73965 | val_0_mse: 0.17328 |  0:00:10s\n",
      "epoch 1  | loss: 0.16103 | val_0_mse: 0.1443  |  0:00:20s\n",
      "epoch 2  | loss: 0.1406  | val_0_mse: 0.13664 |  0:00:29s\n",
      "epoch 3  | loss: 0.12008 | val_0_mse: 0.13728 |  0:00:39s\n",
      "epoch 4  | loss: 0.09625 | val_0_mse: 0.10717 |  0:00:49s\n",
      "epoch 5  | loss: 0.07152 | val_0_mse: 0.07017 |  0:00:59s\n",
      "epoch 6  | loss: 0.05482 | val_0_mse: 0.04687 |  0:01:09s\n",
      "epoch 7  | loss: 0.0521  | val_0_mse: 0.04121 |  0:01:20s\n",
      "epoch 8  | loss: 0.03655 | val_0_mse: 0.03134 |  0:01:30s\n",
      "epoch 9  | loss: 0.03102 | val_0_mse: 0.02541 |  0:01:41s\n",
      "epoch 10 | loss: 0.03408 | val_0_mse: 0.02227 |  0:01:52s\n",
      "epoch 11 | loss: 0.03802 | val_0_mse: 0.02353 |  0:02:02s\n",
      "epoch 12 | loss: 0.02482 | val_0_mse: 0.02529 |  0:02:11s\n",
      "epoch 13 | loss: 0.02963 | val_0_mse: 0.02298 |  0:02:20s\n",
      "epoch 14 | loss: 0.02449 | val_0_mse: 0.03254 |  0:02:29s\n",
      "epoch 15 | loss: 0.02597 | val_0_mse: 0.01613 |  0:02:39s\n",
      "epoch 16 | loss: 0.03027 | val_0_mse: 0.03065 |  0:02:48s\n",
      "epoch 17 | loss: 0.02989 | val_0_mse: 0.01824 |  0:02:58s\n",
      "epoch 18 | loss: 0.02661 | val_0_mse: 0.02011 |  0:03:08s\n",
      "epoch 19 | loss: 0.02238 | val_0_mse: 0.04586 |  0:03:17s\n",
      "epoch 20 | loss: 0.02371 | val_0_mse: 0.01244 |  0:03:27s\n",
      "epoch 21 | loss: 0.02429 | val_0_mse: 0.02265 |  0:03:37s\n",
      "epoch 22 | loss: 0.02838 | val_0_mse: 0.02131 |  0:03:47s\n",
      "epoch 23 | loss: 0.01915 | val_0_mse: 0.04618 |  0:03:56s\n",
      "epoch 24 | loss: 0.01763 | val_0_mse: 0.03079 |  0:04:06s\n",
      "epoch 25 | loss: 0.01848 | val_0_mse: 0.02078 |  0:04:16s\n",
      "epoch 26 | loss: 0.01476 | val_0_mse: 0.01525 |  0:04:25s\n",
      "epoch 27 | loss: 0.01648 | val_0_mse: 0.01063 |  0:04:35s\n",
      "epoch 28 | loss: 0.02176 | val_0_mse: 0.01098 |  0:04:45s\n",
      "epoch 29 | loss: 0.01711 | val_0_mse: 0.00892 |  0:04:54s\n",
      "epoch 30 | loss: 0.0203  | val_0_mse: 0.00863 |  0:05:04s\n",
      "epoch 31 | loss: 0.02018 | val_0_mse: 0.0083  |  0:05:14s\n",
      "epoch 32 | loss: 0.01519 | val_0_mse: 0.02146 |  0:05:24s\n",
      "epoch 33 | loss: 0.01886 | val_0_mse: 0.01091 |  0:05:34s\n",
      "epoch 34 | loss: 0.01627 | val_0_mse: 0.00779 |  0:05:44s\n",
      "epoch 35 | loss: 0.01524 | val_0_mse: 0.02293 |  0:05:55s\n",
      "epoch 36 | loss: 0.01535 | val_0_mse: 0.03668 |  0:06:05s\n",
      "epoch 37 | loss: 0.01547 | val_0_mse: 0.01476 |  0:06:15s\n",
      "epoch 38 | loss: 0.0165  | val_0_mse: 0.02423 |  0:06:25s\n",
      "epoch 39 | loss: 0.01618 | val_0_mse: 0.02104 |  0:06:36s\n",
      "epoch 40 | loss: 0.01815 | val_0_mse: 0.01767 |  0:06:48s\n",
      "epoch 41 | loss: 0.01321 | val_0_mse: 0.02676 |  0:07:00s\n",
      "epoch 42 | loss: 0.01328 | val_0_mse: 0.01211 |  0:07:11s\n",
      "epoch 43 | loss: 0.01179 | val_0_mse: 0.01047 |  0:07:21s\n",
      "epoch 44 | loss: 0.01344 | val_0_mse: 0.01825 |  0:07:32s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_mse = 0.00779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MSE: 0.007745 - Best MSE: 0.007745\n",
      "Model R2 Score: 0.965665 - Best R2 Score: 0.965665\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('tabularML/training'):\n",
    "    os.system('rm -r tabularML/training')\n",
    "else:\n",
    "    os.makedirs('tabularML/training')\n",
    "\n",
    "current_iter = 0\n",
    "\n",
    "best_mse = float('inf')\n",
    "best_model = None\n",
    "best_n_d = None\n",
    "best_n_a = None\n",
    "best_n_step = None\n",
    "best_n_indipendent = None\n",
    "best_n_shared = None\n",
    "best_gamma = None\n",
    "best_batch_size = None\n",
    "\n",
    "for n_d_a, n_step,n_indipendent,n_shared, gamma, epsilon,nums_epochs, batch_sizes in hyperparameters:\n",
    "    current_iter += 1\n",
    "\n",
    "    print(\"\\nIterations {}/{} - Hyperparameters:  batch_sizes={}, nums_epochs={}, n_d={}, n_a={}, n_step={}, n_indipendent={}, n_shared={}, gamma={}, epsilon={}\".format(\n",
    "        current_iter, n_comb, batch_sizes, nums_epochs, n_d_a, n_d_a, n_step, n_indipendent, n_shared, gamma, epsilon ))\n",
    "\n",
    "    model = get_model(n_d_a, n_step, n_indipendent, n_shared, gamma, epsilon)\n",
    "    \n",
    "    log_name = \"batch_size=\"+str(batch_sizes)+\"n_d=\"+str(n_d_a)+\"n_a=\"+str(n_d_a)+\"n_step=\"+str(n_step)+\"n_indipendent=\"+str(n_indipendent)+\"n_shared=\"+str(n_shared)+\"gamma=\"+str(gamma)+\"epsilon=\"+str(epsilon)\n",
    "    \n",
    "    # start tensorboard\n",
    "    writer = SummaryWriter('tabularML/training/'+log_name)\n",
    "    \n",
    "    # train\n",
    "    model.fit(\n",
    "                X_train=X_train,\n",
    "                y_train=Y_train,\n",
    "                eval_set=[(X_val, Y_val)],\n",
    "                eval_metric=['mse'],\n",
    "                # patience: the number of epochs to wait without improvement in validation loss before early stopping (default 10)\n",
    "                patience=10,\n",
    "                # batch_size: the number of samples per batch (default 1024)\n",
    "                batch_size=batch_sizes,\n",
    "                # virtual_batch_size: the number of samples per virtual batch (default 128)\n",
    "                virtual_batch_size=128,\n",
    "                # num_workers: the number of worker processes to use for data loading (default 0)\n",
    "                num_workers=0,\n",
    "                # drop_last: whether to drop the last incomplete batch if the dataset size is not divisible by the batch size (default False)\n",
    "                drop_last=False,\n",
    "                # max_epochs: the maximum number of epochs to train for (default 100)\n",
    "                max_epochs=nums_epochs,\n",
    "            )\n",
    "    \n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, preds)\n",
    "    \n",
    "    writer.add_hparams({'n_d':n_d_a, 'n_a':n_d_a, 'n_step':n_step, 'n_indipendent':n_indipendent, 'n_shared':n_shared, 'gamma':gamma, 'epsilon':epsilon, 'batch_sizes':batch_sizes, 'nums_epochs':nums_epochs }, {'hparam/mse': mse})\n",
    "\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_n_d = n_d_a\n",
    "        best_n_a = n_d_a\n",
    "        best_n_step = n_step\n",
    "        best_n_indipendent = n_indipendent\n",
    "        best_n_shared = n_shared\n",
    "        best_gamma = gamma\n",
    "        best_batch_size = batch_sizes\n",
    "        best_model = copy.deepcopy(model) \n",
    "        \n",
    "    writer.flush()            \n",
    "            \n",
    "    print(\"Model MSE: {:.6f} - Best MSE: {:.6f}\".format(mse, best_mse))\n",
    "    print(\"Model R2 Score: {:.6f} - Best R2 Score: {:.6f}\".format(r2_score(Y_test, preds), r2_score(Y_test, best_model.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
